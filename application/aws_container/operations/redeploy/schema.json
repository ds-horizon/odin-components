{
  "title": "AWS container redeploy operation schema",
  "type": "object",
  "properties": {
    "artifact": {
      "type": "object",
      "description": "Application artifact to deploy, containing the application code, dependencies, and deployment hooks. This specifies which version of your application to deploy and any lifecycle hooks to execute during deployment. Change this to deploy different versions, roll back to previous versions, or update deployment hooks. The artifact version determines what code runs in production and enables versioning and rollback capabilities.",
      "properties": {
        "version": {
          "type": "string",
          "description": "Version identifier of the application artifact to deploy. This determines which version of your application code will be deployed. Change this to deploy different versions, perform rollbacks, or update to new releases. Version must exist in your artifact repository. Using clear versioning enables tracking deployments, auditing changes, and quick rollbacks. Immutable versions ensure consistency across environments."
        },
        "hooks": {
          "type": "object",
          "description": "Deployment lifecycle hooks that execute scripts at specific stages of the deployment process. Hooks enable custom logic for initialization, cleanup, data migration, smoke tests, or integration tasks. Use hooks to prepare environment, migrate databases, run health checks, or clean up resources. Hooks run in order: imageSetup → preDeploy → [deploy] → start → postDeploy → stop. Failed hooks can abort deployment to prevent bad releases. Keep hooks idempotent (safe to run multiple times) and fast (avoid long-running tasks).",
          "properties": {
            "start": {
              "type": "object",
              "description": "Hook that executes after application container starts but before it begins serving traffic. Use for application warm-up, cache preloading, connection pool initialization, or registering with service discovery. This hook runs inside the application container with access to application runtime environment. Failed start hook prevents the application from becoming ready. Keep lightweight to minimize startup time. Start hook blocks readiness probe until completion.",
              "properties": {
                "script": {
                  "type": "string",
                  "description": "File path to the start hook script relative to artifact root (e.g., `start.sh`). Script must exist in the artifact and be executable. Script receives environment variables from container. Exit code 0 indicates success; non-zero fails the hook and prevents application startup. Script runs with same permissions as application process. **Production:** Use relative paths from artifact root. Make scripts executable (chmod +x). Include shebang line (#!/bin/bash). Handle errors explicitly. Log output to stdout/stderr for debugging. Keep scripts in version control with artifact. Test scripts in staging."
                },
                "enabled": {
                  "type": "boolean",
                  "description": "Whether to execute this start hook during deployment. Set to `true` to run the hook, `false` to skip it. Disabling allows temporarily bypassing hook without removing script. Use when hook is optional or needs to be disabled for troubleshooting. Disabled hooks are skipped silently without error. Useful for gradual rollout of new hooks or emergency deployments. **Production:** Keep enabled in production if hook is critical for application startup. Disable only for troubleshooting or when hook is truly optional. Document why hooks are disabled. Re-enable after debugging. Default to `true` for production-critical startup tasks."
                }
              },
              "additionalProperties": false,
              "required": []
            },
            "stop": {
              "type": "object",
              "description": "Hook that executes when application container is stopping, before container termination. Use for graceful shutdown tasks like flushing caches, closing connections, saving state, or deregistering from service discovery. This hook runs inside the application container during termination. Kubernetes waits for hook completion before sending SIGTERM to application (up to terminationGracePeriodSeconds). Failed stop hook is logged but doesn't prevent shutdown. Keep fast to avoid delayed pod termination. **Production:** Use for graceful cleanup (flush buffers, close connections, save state). Keep execution under 15s. Make idempotent since it may run multiple times. Don't rely on stop hook for critical data - it may not run if pod is force-killed. Log execution. Example uses: flush metrics, close DB connections, deregister from registry.",
              "properties": {
                "script": {
                  "type": "string",
                  "description": "File path to the stop hook script relative to artifact root (e.g., `stop.sh`). Script must exist in artifact and be executable. Script has limited time to complete (constrained by terminationGracePeriodSeconds). Exit code doesn't affect shutdown - container terminates regardless. Use for cleanup only, not critical operations. **Production:** Use relative paths. Make scripts fast (<15s). Handle partial execution gracefully. Log to stdout/stderr. Don't depend on stop hook for critical operations - pod can be force-killed. Test with abrupt terminations."
                },
                "enabled": {
                  "type": "boolean",
                  "description": "Whether to execute this stop hook during pod termination. Set to `true` to run cleanup logic, `false` to skip. Disabling allows bypassing hook without removing script. Useful when graceful shutdown isn't required or causes issues. Disabled stop hooks don't affect application - container still terminates normally. Consider disabling if hook causes slow shutdowns or isn't needed. **Production:** Enable if graceful shutdown is important for data integrity or connection cleanup. Disable if hook causes problems or cleanup isn't critical. Document reasoning. Monitor termination times - slow stop hooks delay new pod startup during rolling updates."
                }
              },
              "additionalProperties": false,
              "required": []
            },
            "preDeploy": {
              "type": "object",
              "description": "Hook that executes before new application version is deployed, runs as separate Kubernetes Job. Use for preparation tasks like database migrations, schema updates, cache invalidation, or configuration validation. This hook runs to completion before any new pods are created. Failed preDeploy hook aborts entire deployment to prevent deploying incompatible code. Hook runs in separate pod/container, not in application container. Critical for ensuring environment is ready for new version. Blocking nature prevents race conditions between migrations and new code. **Production:** Use for database migrations, schema changes, data backfills. Make idempotent (safe to retry). Include rollback logic. Test thoroughly - failures block all deployments. Set reasonable timeout (5-30min for migrations). Monitor execution time. Example uses: run database migrations, update external configs, validate prerequisites, warm shared caches.",
              "properties": {
                "script": {
                  "type": "string",
                  "description": "File path to preDeploy hook script relative to artifact root (e.g., `pre-deploy.sh`). Script runs in separate container before deployment. Must be executable and present in artifact. Script receives environment variables configured for deployment. Exit code 0 indicates success and allows deployment to proceed; non-zero aborts entire deployment. Script output logged to Job logs. **Production:** Use for migrations or critical prep work. Make scripts robust with error handling. Include retry logic for transient failures. Log progress verbosely. Test with production-like data volumes. Set appropriate timeouts. Version scripts with application code. Example: `db/migrations/migrate.sh` for database migrations."
                },
                "enabled": {
                  "type": "boolean",
                  "description": "Whether to execute preDeploy hook before deployment. Set to `true` to run preparation tasks, `false` to skip. Disabling bypasses hook without removing script. Use when migrations aren't needed (no schema changes) or for emergency deployments. Skipping migrations when needed causes application errors. Only disable if you're certain environment is compatible with new version. **Production:** Keep enabled for normal deployments if migrations exist. Disable only when deployment has no schema/data changes and you've verified compatibility. Document when/why disabled. Re-enable for next deployment. Never disable to work around migration failures - fix the migration instead."
                },
                "dockerImage": {
                  "type": "string",
                  "description": "Docker image to use for running the preDeploy hook script (e.g., `postgres:15-alpine`, `python:3.11-slim`, application image). Hook runs in container from this image. Choose image with required tools and dependencies for your script (database clients, language runtimes, etc.). Can use same image as application or specialized migration image. Image must have script dependencies installed. Affects hook startup time and resource usage. **Production:** Use lightweight images with only required tools. Pin specific versions (no `latest` tags). For database migrations, use official client images matching your database version. For complex migrations, use application image to ensure code compatibility. Example: `myapp:1.2.3` for app-aware migrations, `postgres:15.2-alpine` for SQL migrations, `python:3.11-slim` for Python scripts."
                }
              },
              "additionalProperties": false,
              "required": []
            },
            "postDeploy": {
              "type": "object",
              "description": "Hook that executes after new application version is successfully deployed and healthy, runs as separate Kubernetes Job. Use for post-deployment tasks like smoke tests, cache warming, sending notifications, updating external systems, or triggering dependent deployments. This hook runs after new pods are healthy and receiving traffic. Failed postDeploy hook logs error but doesn't rollback deployment - new version stays deployed. Hook runs in separate pod/container, not in application container. Useful for verification and notification, not critical deployment steps. Non-blocking nature means failures don't affect deployed application. **Production:** Use for smoke tests, notifications, cache warming, external system updates. Make idempotent. Don't use for critical tasks - failures won't rollback deployment. Include monitoring/alerting integration. Keep execution time reasonable (<5min). Example uses: run smoke tests, send Slack notification, update service registry, warm CDN cache, trigger downstream deployments.",
              "properties": {
                "script": {
                  "type": "string",
                  "description": "File path to postDeploy hook script relative to artifact root (e.g., `tests/smoke.sh`, `hooks/post-deploy.py`). Script runs in separate container after successful deployment. Must be executable and present in artifact. Script receives environment variables and deployment context. Exit code 0 indicates success; non-zero logs error but doesn't affect deployment. Script output logged to Job logs for debugging. **Production:** Use for verification and notification tasks. Include smoke tests to validate deployment. Handle errors gracefully - failures shouldn't require intervention. Log comprehensively for debugging. Keep scripts fast (<5min). Test in staging. Example: `tests/smoke-test.sh` to verify critical endpoints, `hooks/notify.sh` to send deployment notifications."
                },
                "enabled": {
                  "type": "boolean",
                  "description": "Whether to execute postDeploy hook after deployment succeeds. Set to `true` to run verification/notification tasks, `false` to skip. Disabling bypasses hook without removing script. Useful when verification isn't needed or for faster deployments. Disabled postDeploy hooks don't affect deployed application. Consider disabling for hotfix deployments where speed matters. **Production:** Keep enabled for normal deployments to verify success and send notifications. Disable for emergency hotfixes to save time. Document when disabled. Re-enable after emergency. Use for deployment quality gates and team awareness."
                },
                "dockerImage": {
                  "type": "string",
                  "description": "Docker image to use for running the postDeploy hook script (e.g., `alpine:3.18`, `curlimages/curl:latest`, application image). Hook runs in container from this image. Choose image with tools needed for your script (curl for API tests, cloud CLI for updates, etc.). Can use same image as application or specialized test/tool image. Image affects what's available to script. **Production:** Use lightweight images with required tools. For smoke tests, use image with curl/http clients. For cloud operations, use official CLI images (aws-cli, gcloud). Pin specific versions. Example: `curlimages/curl:8.1.0` for HTTP tests, `myapp:1.2.3` for app-aware tests, `alpine:3.18` with custom tools."
                }
              },
              "additionalProperties": false,
              "required": []
            },
            "imageSetup": {
              "type": "object",
              "description": "Hook that executes during Docker image build process, before creating the final container image. Use for custom image preparation like installing system packages, compiling native dependencies, downloading assets, or configuring build-time settings. This hook runs in the image build context with build-time access. Failed imageSetup hook aborts image creation and prevents deployment. Runs once during image build, not during each pod startup. Use for expensive setup that shouldn't repeat per pod. Output becomes part of final image. Increases image build time and potentially image size. **Production:** Use for system dependencies, native compilation, asset preprocessing. Keep build time reasonable (<10min). Minimize image size impact - clean up temporary files. Make deterministic for reproducible builds. Cache wisely. Example uses: install system libs, compile C extensions, download static assets, generate config files.",
              "properties": {
                "script": {
                  "type": "string",
                  "description": "File path to imageSetup script relative to artifact root (e.g., `setup.sh`). Script runs during Docker image build with build context access. Must be executable and present in artifact. Script can modify filesystem - changes persist in final image. Exit code 0 indicates success; non-zero aborts image build. Script runs as root with full image filesystem access. Output logged to build logs. **Production:** Install only required system packages. Clean up temp files and caches (apt-get clean, yum clean all). Pin package versions for reproducibility. Use build caching effectively. Test locally with Docker builds. Keep execution time minimal. Example: `build/install-deps.sh` to install system dependencies, `hooks/compile.sh` to build native extensions."
                },
                "enabled": {
                  "type": "boolean",
                  "description": "Whether to execute imageSetup hook during image build. Set to `true` to run custom build steps, `false` to skip. Disabling bypasses hook without removing script. Use when custom build steps aren't needed (base image has everything) or for faster builds during development. Skipping required setup causes runtime failures. Only disable if base image is sufficient or setup is handled elsewhere. **Production:** Keep enabled if image requires custom build steps. Disable only when switching to pre-built images or base image has all dependencies. Document reasoning. Test image functionality when disabling. Re-enable if runtime errors occur."
                }
              },
              "additionalProperties": false,
              "required": []
            }
          },
          "additionalProperties": false,
          "required": []
        }
      },
      "additionalProperties": false,
      "required": [
        "version"
      ]
    },
    "baseImage": {
      "type": "object",
      "description": "Override the default container base image configuration for this deployment. Allows changing the Docker image runtime environment during redeploy without updating main configuration. Use this to test different base images for language runtimes (e.g., node, python, java), apply security patches to OS distributions, or use different runtime versions. If not specified, uses baseImage from main schema. Changing base image affects runtime environment, installed packages, system libraries, and potential application compatibility. Using official, maintained images ensures security patches and compatibility. Test thoroughly when changing base images as they can introduce breaking changes. **Production:** Override only when necessary (security patches, runtime updates, testing new runtimes). Use specific version tags (e.g., `node:18.20.0-alpine`) rather than `latest` to ensure consistent deployments and avoid unexpected updates. Pin to LTS versions for stability. Test new base images in staging first before production. Ensure compatibility with application code. Document base image changes and reason for override.",
      "properties": {
        "repository": {
          "type": "string",
          "description": "Override the Docker image repository path for this deployment (e.g., `nginx`, `myregistry.io/myapp`). This changes where the container image is pulled from. Use this to switch registries, test different base images from alternative sources, apply custom patches, or use images from private registries. Must be accessible from cluster. If not specified, uses repository from main schema. Using private registries gives you control over image versions and security scanning. **Production:** Use when testing new base images or switching registries for security/compliance. Use private container registries (e.g., ECR, Docker Hub private repos) with vulnerability scanning enabled. Ensure new repository is accessible and contains compatible images. Verify image security scanning is enabled. Ensure images are scanned for CVEs before deployment. Document override reason."
        },
        "tag": {
          "type": "string",
          "description": "Override the specific version tag of the Docker image for this deployment (e.g., `1.21.0`, `v2.3.4-alpine`). This changes which base image version will be used. Use this to test runtime updates, apply security patches to fix CVEs, deploy different versions for testing, or revert to known-good versions after issues. Must exist in repository. If not specified, uses tag from main schema. Using specific tags ensures predictable deployments and enables version tracking. Avoid using `latest` tag as it makes rollbacks difficult and deployments unpredictable. **Production:** Use semantic versioning tags (e.g., `1.2.3`) or commit SHAs for full traceability. Override for security patches (e.g., `node:18.20.1` fixing CVE). Never use `latest` in production as it breaks deployment reproducibility. Test compatibility in staging. Use specific version tags. Document version change reason and testing results."
        }
      },
      "additionalProperties": false,
      "required": []
    },
    "tolerations": {
      "type": "array",
      "description": "Override Kubernetes tolerations for this deployment that allow pods to be scheduled on nodes with matching taints. Tolerations enable pods to run on nodes that would otherwise reject them due to taints (e.g., dedicated nodes, spot instances, GPU nodes). Use this override to temporarily change pod placement for testing on different node types, handling capacity constraints, or responding to infrastructure changes. If not specified, uses tolerations from main schema. Without matching tolerations, pods cannot be scheduled on tainted nodes, potentially causing scheduling failures. Changing tolerations affects where pods can schedule, potentially causing rescheduling during deployment. Useful for temporarily using spot instances, GPU nodes, or dedicated pools. **Production:** Override when testing new node types or handling capacity issues. Use tolerations to schedule workloads on dedicated node groups (e.g., high-memory nodes, spot instances for cost savings). Combine with node selectors or affinity rules for precise placement. Ensure sufficient nodes with matching taints exist. Test placement before production rollout. For critical services, avoid spot instances by not tolerating spot taints. Document override reason. Revert to default after testing or restore capacity.",
      "items": {
        "type": "object",
        "properties": {
          "key": {
            "type": "string",
            "description": "Override the taint key that this toleration matches (e.g., `node.kubernetes.io/spot`, `dedicated`). This identifies which node taint the pod can tolerate. Must match taint on target nodes. Override this to schedule on different node types during this deployment. Change this to match specific taints applied to your nodes. Matching the correct key allows pods to be scheduled on nodes with that taint. **Production:** Use to temporarily access different node pools. Use consistent taint keys across your cluster. Verify nodes with this taint have sufficient capacity. Common patterns: `workload-type=batch`, `instance-type=spot`, `node-role=gpu`. Document override purpose."
          },
          "operator": {
            "type": "string",
            "description": "Override how to match the taint. `Equal` matches exact key-value pairs, while `Exists` matches any value for the key. Use `Equal` when you need precise matching (e.g., only spot instances with specific characteristics). Use `Exists` for broader matching (e.g., any spot instance). Override to broaden or narrow node selection for this deployment. **Default:** `Equal` (requires value to be specified). **Production:** Use `Equal` for explicit control and precise placement, `Exists` for flexible scheduling across node types or during capacity issues."
            ,
            "enum": [
              "Equal",
              "Exists"
            ]
          },
          "value": {
            "type": "string",
            "description": "Override the taint value that must match when operator is `Equal` (e.g., `true`, `spot`, `gpu`). This provides fine-grained control over which tainted nodes the pod can tolerate. Change this to match specific taint values on your nodes. Required when operator is `Equal`, ignored when operator is `Exists`. Override to target specific tainted nodes for this deployment. **Production:** Use meaningful values that indicate node characteristics (e.g., `true` for dedicated nodes, `spot` for spot instances). Match production taint values. Verify taint configuration on target nodes."
          },
          "effect": {
            "type": "string",
            "description": "Override the taint effect to tolerate. `NoSchedule` prevents new pods from scheduling, `PreferNoSchedule` avoids scheduling but allows if necessary, `NoExecute` evicts running pods that don't tolerate. Matching the effect type determines when the toleration applies. Use `NoSchedule` for dedicated nodes, `NoExecute` for critical workload isolation. Override to change scheduling behavior for this deployment. **Default:** `NoSchedule` (pods won't schedule without toleration). **Production:** Use `NoSchedule` for dedicated workloads, `NoExecute` for strict isolation where incompatible pods must be evicted. Avoid `PreferNoSchedule` in production as it provides weak guarantees and unpredictable behavior.",
            "enum": [
              "NoSchedule",
              "PreferNoSchedule",
              "NoExecute"
            ]
          }
        },
        "allOf": [
          {
            "if": {
              "properties": {
                "operator": {
                  "const": "Exists"
                }
              },
              "required": ["operator"]
            },
            "then": {
              "required": ["key"]
            },
            "else": {
              "required": ["key", "value"]
            }
          }
        ]
      }
    },
    "nodeSelector": {
      "type": "object",
      "description": "Override Kubernetes node selector labels for this deployment, constraining pod placement to specific nodes. Use to test on specific node types, isolate deployments to dedicated hardware, or work around infrastructure issues. If not specified, uses nodeSelector from main schema or no selector. Node selector is stricter than tolerations - pods ONLY schedule on matching nodes. Useful for A/B testing on different instance types, using specific hardware (SSD, high memory), or emergency capacity management. Misconfigured selectors can prevent pod scheduling entirely. **Production:** Override for testing new instance types or handling capacity constraints. Ensure sufficient matching nodes exist (at least replicas + 1 for HA). Verify nodes have required labels with `kubectl get nodes -l key=value`. Monitor pod scheduling events. Document override reason. Remove override after testing. Example: `{\"node.kubernetes.io/instance-type\": \"c5.xlarge\"}` to target specific instances.",
      "properties": {
      }
    },
    "extraEnvVars": {
      "type": "object",
      "description": "Override or add additional environment variables to inject into the container at runtime (e.g., `LOG_LEVEL: debug`, `FEATURE_FLAGS: enabled`). Environment variables configure application behavior without code changes. Use this to pass configuration values, feature flags, API endpoints, or debugging settings. Use this override to change feature flags, adjust log levels for debugging, modify API endpoints for testing, or apply hotfix configurations. If not specified, uses extraEnvVars from main schema. Each key-value pair becomes available as an environment variable inside the container. New keys are added, existing keys are overridden. Changing these requires pod restart to take effect. Environment changes require pod restart. Avoid storing secrets here; use Kubernetes Secrets instead. Useful for enabling debug mode, feature toggles, or emergency configuration changes without redeploying artifacts. **Production:** Keep production-specific values here (e.g., `NODE_ENV: production`). Use for non-sensitive configuration only. For secrets like API keys or passwords, use proper secret management (Kubernetes Secrets, AWS Secrets Manager). Override for debugging (LOG_LEVEL=debug), feature flags (FEATURE_X_ENABLED=true), or emergency configuration changes. Keep changes minimal and documented. Monitor application behavior after env changes. Revert non-essential overrides after debugging. Example: `{\"LOG_LEVEL\": \"debug\", \"FEATURE_NEW_API\": \"true\"}` for troubleshooting.",
      "properties": {
      }
    },
    "resources": {
      "type": "object",
      "description": "Override CPU and memory resource allocation for the container, defining both requests (guaranteed resources) and limits (maximum allowed). Resource configuration determines pod scheduling, node selection, and prevents resource starvation. Use this override to test performance with different resources, handle increased load temporarily, troubleshoot resource-related issues, or perform load testing. If not specified, uses resources from main schema. Configure this based on application profiling and load testing. Requests that are too low cause performance issues; limits that are too low cause OOM kills and throttling. Setting requests = limits provides guaranteed QoS but reduces scheduling flexibility. Changing resources affects pod scheduling, performance, and cost. Useful for load testing, performance tuning, or emergency capacity adjustments. Can override just requests, just limits, or both. **Production:** Start with profiled values from staging. Override to handle temporary load spikes (increase resources) or test performance (tune resources). Set requests at p50 usage, limits at p95. For critical services, use guaranteed QoS (requests = limits). Monitor actual usage and adjust based on metrics to avoid over-provisioning costs or under-provisioning instability. Test in staging first. Monitor actual usage vs allocated. Revert after testing or make permanent in main schema if change is beneficial. Be aware of cost implications for large resource increases.",
      "additionalProperties": false,
      "properties": {
        "requests": {
          "type": "object",
          "description": "Override guaranteed minimum resources allocated to the container by Kubernetes scheduler. These resources are reserved on the node even if not fully utilized. The scheduler uses requests to determine which nodes can accommodate the pod. Use this override to temporarily adjust baseline resource allocation for testing or capacity management. Set requests based on baseline resource needs under normal load. Too low causes performance degradation; too high wastes cluster capacity and increases costs. Requests also determine pod QoS class (Guaranteed if requests=limits, Burstable if requests<limits). Affects pod scheduling and QoS. If not specified, uses requests from main schema. **Production:** Set requests to p50 (median) observed usage from production profiling. This ensures pods get scheduled on nodes with sufficient capacity while avoiding resource waste. Override to test different resource profiles or handle capacity constraints. Ensure cluster has sufficient resources for new requests. Monitor scheduling and performance.",
          "additionalProperties": false,
          "properties": {
            "cpu": {
              "type": "string",
              "description": "Override minimum CPU cores guaranteed to the container (e.g., `0.5` = 500 millicores, `2` = 2 cores, `500m`). CPU requests determine scheduling and share of CPU time when contention occurs. Use this to test performance with different CPU allocation or handle CPU-intensive workloads temporarily. Set this based on application's baseline CPU needs. Too low causes slow response times under load; too high wastes cluster resources and may prevent pod scheduling. CPU is a compressible resource, so pods can use less than requested. Units: whole numbers (cores) or decimals, can use `m` suffix (e.g., `500m` = 0.5 cores). Affects pod scheduling and CPU shares. **Production:** Profile CPU usage under normal load. Set requests to p50 CPU usage. For latency-sensitive services, consider p75 to ensure headroom. Increase for CPU-intensive tasks, decrease for testing lower resource profiles. Monitor CPU usage and throttling."
            },
            "memory": {
              "type": "string",
              "description": "Override minimum memory guaranteed to the container (e.g., `512Mi`, `1Gi`, `2G`). Memory requests determine scheduling and guarantee this amount is available. Use this to test memory requirements or handle memory-intensive operations temporarily. Set based on application's baseline memory footprint. Too low risks OOM kills even with higher limits; too high wastes resources. Memory is incompressible - if app needs more than available, it will be killed. Units: `Mi` (mebibytes, 1024-based), `Gi` (gibibytes), `M` (megabytes, 1000-based), `G` (gigabytes). Affects pod scheduling. **Production:** Profile memory usage after app warmup. Set requests to p50 memory usage. For memory-intensive apps, consider p75 to account for gradual memory growth. Increase for memory-intensive deployments, decrease carefully while monitoring for OOM. Profile actual usage."
            }
          },
          "required": []
        },
        "limits": {
          "type": "object",
          "description": "Override maximum resources the container is allowed to use. Kubernetes enforces these limits; exceeding memory limit causes OOM kill, exceeding CPU limit causes throttling. Limits protect the cluster from runaway containers but can cause performance issues if set too low. Without limits, containers can consume all node resources. Setting limits = requests provides guaranteed QoS but less flexibility. Setting limits > requests allows bursting but provides lower QoS guarantees. Use this override to test with different resource ceilings, prevent resource hogging, or allow more headroom temporarily. Affects throttling and OOM behavior. If not specified, uses limits from main schema. **Production:** Set limits to p95-p99 observed usage to handle traffic spikes. For critical services, set limits 20-50% above requests to allow bursting. Monitor throttling metrics and OOM kills to tune limits appropriately. Override to prevent runaway resource usage or allow more headroom for peak load. Monitor throttling and OOM kills.",
          "additionalProperties": false,
          "properties": {
            "cpu": {
              "type": "string",
              "description": "Override maximum CPU cores the container can use (e.g., `1`, `2`, `4`, `1000m`). When exceeded, container is throttled (slowed down), not killed. CPU throttling increases latency and reduces throughput. Use this to prevent CPU throttling during high load or test throttling behavior. Set based on maximum expected CPU needs during peak load. Setting this too low causes performance degradation during traffic spikes; too high allows one container to starve others. For latency-sensitive workloads, set limits high enough to avoid throttling. Units: same as requests - whole numbers or decimals, can use `m` suffix. CPU limit exceedance causes throttling, not pod kill. **Production:** Set to p95-p99 CPU usage observed under peak load. For latency-critical services, consider setting limits 2-3x requests to avoid throttling. Monitor CPU throttling metrics. Increase to eliminate throttling during peak load, decrease to limit blast radius. Monitor CPU throttling metrics."
            },
            "memory": {
              "type": "string",
              "description": "Override maximum memory the container can use (e.g., `1Gi`, `2Gi`, `4G`). When exceeded, container is killed with OOM (Out Of Memory) error and restarted. Memory limits are hard boundaries - exceeding them is fatal. Use this to prevent OOM kills during high memory usage or test OOM behavior. Set based on maximum memory footprint under peak load plus safety margin. Too low causes frequent OOM kills and service instability; too high allows memory leaks to impact node stability. Unlike CPU, memory cannot be compressed, so limits must accommodate peak usage. Units: same as requests. Memory limit exceedance kills pod. **Production:** Set to p95-p99 memory usage with 20-30% safety margin. Monitor OOM kill events. For apps with memory leaks or growing memory, implement regular restarts or fix leaks rather than just increasing limits. Increase carefully to prevent OOM kills, decrease to limit memory usage. Monitor OOM events closely. Always maintain safety margin above requests."
            }
          },
          "required": []
        }
      },
      "required": []
    },
    "replicas": {
      "type": "integer",
      "description": "Override number of pod replicas for this deployment. Use to temporarily scale up for load testing, scale down for resource conservation, or adjust capacity quickly without updating main configuration. If not specified, uses replicas from main schema. Changing replicas affects availability, capacity, and cost. Useful for handling traffic spikes, performing capacity tests, or emergency scaling. Kubernetes redistributes traffic across new replica count. **Production:** Override to handle sudden traffic increases (scale up) or reduce costs temporarily (scale down carefully). Ensure minimum replicas for HA (at least 2, preferably 3+ for critical services). For load testing, scale up gradually while monitoring. For scale down, verify reduced capacity can handle current load. Consider using HPA for automatic scaling instead of manual overrides. Monitor pod health and traffic distribution after replica changes. Example: increase from 3 to 10 for Black Friday traffic, decrease from 10 to 2 during maintenance window."
    },
    "strategy": {
      "type": "object",
      "description": "Deployment strategy defining how to roll out the new version with zero downtime and risk mitigation. Strategy controls traffic switching, canary analysis, and rollback behavior. Use blue-green for instant traffic cutover with quick rollback capability. Critical for production deployments to minimize risk and enable fast rollback. Strategy choice affects deployment time, risk level, and rollback speed. Blue-green deploys complete new version before switching traffic, enabling instant rollback. **Production:** Always use blue-green strategy for production deployments. Configure canary analysis to catch errors before full traffic switch. Enable auto-routing only after thorough testing in staging. Set appropriate error thresholds based on application's error rate. Plan rollback procedures. Document strategy configuration and test rollback process.",
      "properties": {
        "name": {
          "type": "string",
          "description": "Deployment strategy type. Currently supports `blue-green` for zero-downtime deployments with instant rollback. Blue-green deploys new version (green) alongside existing version (blue), then switches traffic atomically. Enables instant rollback by switching back to blue. New versions must be fully healthy before traffic switch. **Production:** Use `blue-green` for all production deployments. Ensures zero downtime and instant rollback capability. Test strategy in staging to understand timing and behavior. More strategies may be added in future (rolling, canary).",
          "enum": [
            "blue-green"
          ]
        },
        "config": {
          "type": "object",
          "description": "Strategy-specific configuration options. For blue-green strategy, controls traffic routing automation, canary analysis, and passive environment cleanup. Configuration determines deployment risk level and automation level. Use to fine-tune deployment safety vs speed trade-off. **Production:** Configure carefully based on application's error tolerance and rollback requirements. Enable canary for critical services. Set conservative error thresholds. Test configuration in staging before production use."
        }
      },
      "allOf": [
        {
          "if": {
            "properties": {
              "name": {
                "const": "blue-green"
              }
            }
          },
          "then": {
            "properties": {
              "config": {
                "properties": {
                  "autoRouting": {
                    "type": "boolean",
                    "description": "Whether to automatically route traffic to new version after successful deployment and canary analysis. When `true`, traffic switches automatically if deployment is healthy and canary passes (if enabled). When `false`, requires manual traffic switch, allowing human verification before routing production traffic. Automatic routing speeds deployment but requires confidence in health checks and canary analysis. Manual routing adds safety gate but requires human intervention and delays release. **Production:** Use `false` for critical services requiring human verification before traffic switch. Use `true` for well-tested services with reliable canary analysis to enable faster deployments. Start with manual routing, move to automatic after building confidence. Always verify canary analysis is working correctly before enabling auto-routing."
                  },
                  "canary": {
                    "type": "object",
                    "description": "Canary analysis configuration that gradually shifts traffic to new version while monitoring for errors. Canary reduces risk by exposing only small percentage of traffic initially, catching issues before full rollout. Deployment automatically rolls back if error threshold is exceeded. Use canary for risk-sensitive deployments to catch issues early with minimal user impact. Canary adds deployment time but significantly reduces blast radius of bad releases. **Production:** Enable for all critical production services. Configure conservative error thresholds (start with 1-2% error rate increase). Use multiple steps (3-5) with sufficient duration (5-10min per step) to collect meaningful data. Monitor canary metrics during deployments. Test canary rollback behavior in staging.",
                    "properties": {
                      "enabled": {
                        "type": "boolean",
                        "description": "Whether to enable canary analysis during deployment. When `true`, traffic gradually shifts through configured steps with error monitoring. When `false`, traffic switches instantly after deployment (higher risk). Canary analysis catches issues with small traffic percentage before full rollout. Adds deployment time (steps × duration) but dramatically reduces incident blast radius. **Production:** Enable for critical services handling important traffic. Start with canary enabled for all services, disable only for non-critical services after risk assessment. Canary is essential for services where bad releases cause customer impact or revenue loss. Monitor canary effectiveness - if it never catches issues, thresholds may be too permissive."
                      },
                      "errorThreshold": {
                        "type": "object",
                        "description": "Maximum acceptable error rate during canary analysis before automatic rollback. Threshold determines sensitivity vs false positives trade-off. If new version exceeds threshold during any canary step, deployment automatically rolls back to previous version. Use to define acceptable quality standards for production traffic. Threshold too tight causes false rollbacks; too loose misses real issues. Based on application's baseline error rate. **Production:** Set threshold slightly above baseline error rate (e.g., if baseline is 0.5%, set threshold to 1-2%). Account for normal variance. Monitor false rollback rate - adjust if too many false positives. For critical services, prefer tight thresholds (fail fast). Document threshold reasoning.",
                        "properties": {
                          "metric": {
                            "type": "string",
                            "description": "Error measurement type: `absolute` for raw error count per time window, `percentage` for error rate as percentage of requests. Percentage is typically more useful as it accounts for traffic volume. Absolute count good for very low traffic or specific error count limits. Use percentage for most services to handle varying traffic levels. **Production:** Use `percentage` for most services (e.g., 2% error rate). Use `absolute` only for very specific cases like zero-tolerance for certain error types. Percentage adapts to traffic volume automatically.",
                            "enum": [
                              "absolute",
                              "percentage"
                            ]
                          },
                          "value": {
                            "type": "number",
                            "description": "Error threshold value, interpreted based on metric type. For `percentage`: error rate as percentage (e.g., `2` = 2% error rate). For `absolute`: number of errors per time window (e.g., `10` = 10 errors). Exceeding this value triggers automatic rollback. Set based on application's acceptable error rate and baseline error rate. **Production:** For percentage metric, use 1-3% above baseline (e.g., if baseline is 0.5%, use 1.5-2%). For absolute metric, set based on acceptable error count. Start conservative, relax if too many false rollbacks. Example: `2` with percentage metric means rollback if error rate exceeds 2%."
                          }
                        },
                        "additionalProperties": false
                      },
                      "steps": {
                        "type": "object",
                        "description": "Canary rollout step configuration defining how traffic gradually shifts from old to new version. Steps control rollout speed and granularity. More steps with longer duration provides better error detection but slower rollout. Fewer steps with shorter duration deploys faster but higher risk. Each step shifts traffic by weight percentage, waits duration seconds, then checks error threshold. **Production:** Use 3-5 steps for balance of safety and speed. Each step: 5-10min duration to collect meaningful metrics. Weight: 10-25% per step. Example: 5 steps × 20% weight × 5min = 25min gradual rollout. Adjust based on traffic volume (high traffic needs less time per step).",
                        "properties": {
                          "weight": {
                            "type": "number",
                            "description": "Percentage of traffic to shift in each canary step (e.g., `20` means shift 20% of traffic per step). Steps continue until 100% traffic is shifted or error threshold exceeded. Weight determines rollout granularity: smaller weight = more gradual (safer), larger weight = faster (riskier). Total steps = 100 / weight. Must divide evenly into 100 for predictable behavior. **Production:** Use 10-25% weight for most services (4-10 steps total). Use 10% (10 steps) for critical services requiring maximum safety. Use 25% (4 steps) for less critical services. Never use 100% (defeats canary purpose). Example: 20% weight = 5 steps (20%, 40%, 60%, 80%, 100%)."
                          },
                          "count": {
                            "type": "number",
                            "description": "Number of canary steps to execute during rollout. Each step shifts traffic by weight percentage, waits duration, then checks errors. More steps provide finer-grained rollout (safer) but longer total deployment time. Fewer steps deploy faster but with less granularity. Total deployment time = count × duration. If specified, overrides weight-based step calculation. **Production:** Use 3-5 steps for balance. Use 5-10 steps for critical services. Minimum 3 for meaningful canary analysis. Calculate: count × duration = acceptable deployment time. Example: 5 steps × 5min = 25min total canary phase.",
                            "minimum": 1
                          },
                          "duration": {
                            "type": "number",
                            "description": "Seconds to wait at each canary step before proceeding to next step or checking error threshold. Duration must be long enough to collect statistically significant error data. Too short causes false positives (insufficient data); too long delays deployment unnecessarily. Depends on traffic volume: high traffic needs less time. Minimum viable: enough requests for meaningful error percentage. **Production:** Use 300-600s (5-10min) per step for most services. Use 180s (3min) for high-traffic services (thousands RPS). Use 600s+ (10min+) for low-traffic services. Calculate: duration should capture at least 1000-10000 requests for statistical significance. Example: 300s with 100 RPS = 30,000 requests per step (good sample size)."
                          }
                        },
                        "additionalProperties": false
                      }
                    },
                    "additionalProperties": false
                  },
                  "passiveDownscale": {
                    "type": "object",
                    "description": "Automatic downscaling of old (passive) version after successful traffic switch to new (active) version. Reduces resource usage and costs by removing inactive pods. When enabled, waits specified delay after traffic switch, then scales down old version. Delay allows quick rollback if issues discovered immediately after switch. Disabling keeps old version running (increases costs but enables faster manual rollback). **Production:** Enable for cost optimization after confident in deployment. Set delay to 300-600s (5-10min) to allow quick rollback window. Disable for critical deployments where instant rollback capability is required. Keep disabled until confident in deployment process. Monitor for issues during delay window. Re-enable old version if problems detected.",
                    "properties": {
                      "enabled": {
                        "type": "boolean",
                        "description": "Whether to automatically downscale old version after traffic switch. When `true`, scales down old version after delay period, saving resources. When `false`, keeps old version running at full capacity for manual rollback. Disabling increases costs (double resources) but provides instant rollback capability. **Production:** Enable after confident in deployment and canary analysis. Start with disabled to maintain rollback capability. Enable once deployment process is mature and canary reliably catches issues. For critical services, keep disabled to ensure instant rollback option."
                      },
                      "delay": {
                        "type": "number",
                        "description": "Seconds to wait after traffic switch before downscaling old version. Provides window for detecting post-switch issues and performing quick rollback. Delay balances cost (longer = more waste) vs safety (longer = more time to detect issues). During delay, both versions run consuming double resources. After delay, old version scales down and rollback requires redeployment. **Production:** Use 300-600s (5-10min) for most services. Use 600-900s (10-15min) for critical services. Use 180-300s (3-5min) for well-tested services. Monitor for issues during delay window. If issues are typically found within 5min, use 600s delay. Example: 600s provides 10min to detect issues and rollback before old version is removed."
                      }
                    },
                    "additionalProperties": false
                  }
                },
                "additionalProperties": false
              }
            }
          }
        }
      ],
      "required": [
        "name",
        "config"
      ]
    },
    "tags": {
      "type": "object",
      "description": "Override or add custom Kubernetes labels to apply to all created resources (Deployments, Services, Pods, ConfigMaps) during this deployment. Labels enable resource organization, selection, and monitoring queries. Use labels for environment identification (env: production), team ownership (team: platform), cost allocation (cost-center: engineering), or feature flags. Use this override to tag deployment with metadata like release version, deployment reason, owner, or tracking identifiers. If not specified, uses tags from main schema. Labels must be key-value string pairs following Kubernetes label naming rules. These labels are added to all resources created by this deployment, making them queryable via kubectl and visible in monitoring systems. New labels are added, existing labels are overridden. Change this to add organizational metadata, enable resource grouping, or support monitoring/cost tracking. Labels don't affect functionality but are crucial for operations and cost management. Labels enable resource querying, monitoring filtering, and operational tracking. Useful for tagging canary deployments, A/B tests, emergency hotfixes, or tracking deployment attribution. **Production:** Include essential labels: `environment` (prod/staging/dev), `team` (owning team), `service` (service name), `version` (app version), `cost-center` (for billing). Use consistent label keys across organization. Avoid high-cardinality values in labels (like timestamps, user IDs). Add deployment-specific labels (deployment-id, release-version, deployed-by, reason). Keep labels consistent with organizational standards. Use for deployment tracking and troubleshooting. Example: `{\"environment\": \"production\", \"team\": \"platform\", \"service\": \"api\", \"criticality\": \"high\", \"deployment-id\": \"deploy-12345\", \"version\": \"1.2.3\", \"deployed-by\": \"jenkins\", \"change-ticket\": \"JIRA-456\"}`. Labels enable queries like `kubectl get pods -l environment=production,team=platform` and Prometheus queries `up{environment=\"production\"}`. Enable filtering in monitoring: `rate(requests[5m]){version=\"1.2.3\"}`. Avoid high-cardinality values (timestamps, unique IDs per pod) - use deployment-level identifiers only.",
      "properties": {
      }
    }
  },
  "additionalProperties": false,
  "required": [
    "artifact"
  ]
}
