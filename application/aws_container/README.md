## GCP Container Flavor

Deploy and perform operations on your application in EKS

## Operations
- [redeploy](operations/redeploy)

### AWS container flavour schema

#### Properties

| Property       | Type                     | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|----------------|--------------------------|----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `baseImage`    | [object](#baseimage)     | **Yes**  | Container base image configuration specifying the Docker image to run. This defines the runtime environment for your application. Change this to use different base images for language runtimes (e.g., node, python, java) or OS distributions. Using official, maintained images ensures security patches and compatibility. **Production:** Use specific version tags (e.g., `node:18.20.0-alpine`) rather than `latest` to ensure consistent deployments and avoid unexpected updates. Pin to LTS versions for stability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| `extraEnvVars` | [object](#extraenvvars)  | **Yes**  | Additional environment variables to inject into the container at runtime (e.g., `LOG_LEVEL: debug`, `FEATURE_FLAGS: enabled`). Environment variables configure application behavior without code changes. Use this to pass configuration values, feature flags, API endpoints, or debugging settings. Each key-value pair becomes available as an environment variable inside the container. Changing these requires pod restart to take effect. Avoid storing secrets here; use Kubernetes Secrets instead. **Production:** Keep production-specific values here (e.g., `NODE_ENV: production`). Use for non-sensitive configuration only. For secrets like API keys or passwords, use proper secret management (Kubernetes Secrets, AWS Secrets Manager).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| `ports`        | [object](#ports)[]       | **Yes**  | Network ports exposed by the container for external access. Defines how traffic reaches your application through Kubernetes Services and Ingress. Each port configuration creates a service endpoint and routing rule. Configure this to expose HTTP APIs, gRPC services, metrics endpoints, or other network interfaces. At least one port must be defined for the application to be accessible. Multiple ports enable serving different protocols or separating concerns (e.g., API on 8080, metrics on 9090). Changes to ports require service updates and may cause brief downtime. **Production:** Expose only necessary ports. Use standard ports where possible (80/443 for HTTP/HTTPS). Separate admin/metrics ports from application ports. Ensure port configurations match your ingress/load balancer setup.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| `probes`       | [object](#probes)        | **Yes**  | Health check probes that monitor container health and readiness. Kubernetes uses probes to determine when to route traffic (readiness), restart containers (liveness), and when application has finished starting (startup). Configure probes to prevent routing traffic to unhealthy instances, automatically recover from failures, and handle slow-starting applications. Properly tuned probes improve reliability but aggressive probes can cause false-positive restarts. Missing probes risk serving traffic to broken instances or never recovering from failures. **Production:** Always configure all three probe types. Tune thresholds and timeouts based on actual application behavior under load. Monitor probe failure rates to detect issues early.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| `replicas`     | integer                  | **Yes**  | Number of identical pod replicas to run for the application. Replicas provide high availability, load distribution, and increased capacity. More replicas improve availability (survive pod/node failures) and handle higher traffic loads through load balancing. Change this based on traffic requirements, availability needs, and cost constraints. Each replica consumes resources (CPU, memory) specified in resources section. Too few replicas risk availability and capacity issues; too many waste resources and increase costs. Kubernetes distributes replicas across nodes (when possible) for fault tolerance. Scaling replicas up/down causes brief traffic redistribution but no downtime. **Production:** Use minimum 2 replicas for availability (survive single pod failure). Use 3+ for critical services (survive node failure). Scale based on traffic: monitor CPU/memory usage and latency. For autoscaling, this sets minimum replicas. Calculate: (peak RPS / single pod capacity) × 1.5 for headroom. Example: if one pod handles 100 RPS and peak is 500 RPS, use 8 replicas (500/100 × 1.5). Always maintain extra capacity for rolling updates and failures.                                                                                                                                   |
| `resources`    | [object](#resources)     | **Yes**  | CPU and memory resource allocation for the container, defining both requests (guaranteed resources) and limits (maximum allowed). Resource configuration determines pod scheduling, node selection, and prevents resource starvation. Configure this based on application profiling and load testing. Requests that are too low cause performance issues; limits that are too low cause OOM kills and throttling. Setting requests = limits provides guaranteed QoS but reduces scheduling flexibility. **Production:** Start with profiled values from staging. Set requests at p50 usage, limits at p95. For critical services, use guaranteed QoS (requests = limits). Monitor actual usage and adjust based on metrics to avoid over-provisioning costs or under-provisioning instability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| `tags`         | [object](#tags)          | **Yes**  | Custom Kubernetes labels to apply to all created resources (Deployments, Services, Pods, ConfigMaps). Labels enable resource organization, selection, and monitoring queries. Use labels for environment identification (env: production), team ownership (team: platform), cost allocation (cost-center: engineering), or feature flags. Labels must be key-value string pairs following Kubernetes label naming rules. These labels are added to all resources created by this deployment, making them queryable via kubectl and visible in monitoring systems. Change this to add organizational metadata, enable resource grouping, or support monitoring/cost tracking. Labels don't affect functionality but are crucial for operations and cost management. **Production:** Include essential labels: `environment` (prod/staging/dev), `team` (owning team), `service` (service name), `version` (app version), `cost-center` (for billing). Use consistent label keys across organization. Avoid high-cardinality values in labels (like timestamps, user IDs). Example: `{"environment": "production", "team": "platform", "service": "api", "criticality": "high"}`. Labels enable queries like `kubectl get pods -l environment=production,team=platform` and Prometheus queries `up{environment="production"}`. |
| `tolerations`  | [object](#tolerations)[] | No       | Kubernetes tolerations that allow pods to be scheduled on nodes with matching taints. Tolerations enable pods to run on nodes that would otherwise reject them due to taints (e.g., dedicated nodes, spot instances, GPU nodes). Configure this to place pods on specialized node pools or to avoid certain nodes. Without matching tolerations, pods cannot be scheduled on tainted nodes, potentially causing scheduling failures. **Production:** Use tolerations to schedule workloads on dedicated node groups (e.g., high-memory nodes, spot instances for cost savings). Combine with node selectors or affinity rules for precise placement. For critical services, avoid spot instances by not tolerating spot taints.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |

#### baseImage

Container base image configuration specifying the Docker image to run. This defines the runtime environment for your application. Change this to use different base images for language runtimes (e.g., node, python, java) or OS distributions. Using official, maintained images ensures security patches and compatibility. **Production:** Use specific version tags (e.g., `node:18.20.0-alpine`) rather than `latest` to ensure consistent deployments and avoid unexpected updates. Pin to LTS versions for stability.

##### Properties

| Property     | Type   | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|--------------|--------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `repository` | string | **Yes**  | The Docker image repository path (e.g., `nginx`, `myregistry.io/myapp`). This specifies where to pull the container image from. Change this to use images from private registries or different public repositories. Using private registries gives you control over image versions and security scanning. **Production:** Use private container registries (e.g., ECR, Docker Hub private repos) with vulnerability scanning enabled. Ensure images are scanned for CVEs before deployment.                                                                                                               |
| `tag`        | string | **Yes**  | The specific version tag of the Docker image (e.g., `1.21.0`, `v2.3.4-alpine`). This determines which version of the application will be deployed. Change this to deploy different versions of your application or roll back to previous versions. Using specific tags ensures predictable deployments and enables version tracking. Avoid using `latest` tag as it makes rollbacks difficult and deployments unpredictable. **Production:** Use semantic versioning tags (e.g., `1.2.3`) or commit SHAs for full traceability. Never use `latest` in production as it breaks deployment reproducibility. |

#### extraEnvVars

Additional environment variables to inject into the container at runtime (e.g., `LOG_LEVEL: debug`, `FEATURE_FLAGS: enabled`). Environment variables configure application behavior without code changes. Use this to pass configuration values, feature flags, API endpoints, or debugging settings. Each key-value pair becomes available as an environment variable inside the container. Changing these requires pod restart to take effect. Avoid storing secrets here; use Kubernetes Secrets instead. **Production:** Keep production-specific values here (e.g., `NODE_ENV: production`). Use for non-sensitive configuration only. For secrets like API keys or passwords, use proper secret management (Kubernetes Secrets, AWS Secrets Manager).

| Property | Type | Required | Description |
|----------|------|----------|-------------|

#### ports

##### Properties

| Property     | Type    | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|--------------|---------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `name`       | string  | **Yes**  | Descriptive name for the port (e.g., `http`, `https`, `grpc`, `metrics`). Used to identify the port in Kubernetes resources and service discovery. Must be unique within the pod and follow DNS label standards (lowercase alphanumeric and hyphens). Use meaningful names that indicate the port's purpose. Enables referring to ports by name in readiness/liveness probes and service definitions. **Production:** Use standard naming conventions: `http` for HTTP APIs, `https` for HTTPS, `grpc` for gRPC, `metrics` for Prometheus metrics, `health` for health checks. Consistent naming improves maintainability and monitoring setup.                                                                           |
| `path`       | string  | **Yes**  | URL path prefix for routing requests to this port (e.g., `/api`, `/v1`, `/health`, `/`). Used by ingress controllers to route HTTP requests based on path. All requests matching this path prefix will be forwarded to this port. Change this to implement path-based routing for different services or API versions. Path must start with `/`. Overlapping paths may cause routing conflicts depending on ingress controller configuration. **Production:** Use specific paths for different services (/api, /admin) or versions (/v1, /v2). Root path `/` catches all unmatched requests. Ensure paths don't overlap unless intentional. Test routing rules thoroughly as misconfigurations can expose wrong endpoints. |
| `port`       | integer | **Yes**  | External port number exposed by the Kubernetes Service (e.g., `80`, `8080`, `443`). This is the port clients connect to when accessing your service. Change this to use standard ports (80 for HTTP, 443 for HTTPS) or custom ports based on your networking requirements. The port must be unique within the service and not conflict with other services. Consider firewall rules and load balancer configurations when choosing ports. **Production:** Use standard ports (80/443) when possible for external-facing services. For internal services, use ports that align with your organization's port allocation strategy. Avoid ephemeral port range (32768-65535).                                                |
| `protocol`   | string  | **Yes**  | Network protocol for the port (e.g., `TCP`, `UDP`, `SCTP`). Determines how traffic is transmitted to the container. Use `TCP` for HTTP/HTTPS, gRPC, database connections (most common). Use `UDP` for DNS, streaming protocols, or custom UDP services. Protocol must match what your application expects; mismatch causes connection failures. Most web services use TCP. **Production:** Use `TCP` for web services and APIs. Only use `UDP` if your application specifically requires it. Verify protocol compatibility with load balancers and ingress controllers.                                                                                                                                                   |
| `targetPort` | integer | **Yes**  | Port number where the container is actually listening (e.g., `8080`, `3000`, `8443`). This is the port your application code binds to inside the container. Must match the port configured in your application. Mismatch between targetPort and application's listening port causes connection refused errors. Allows mapping external port to different internal port (e.g., external 80 → internal 8080). **Production:** Use non-privileged ports (>1024) inside containers for security. Common choices: 8080 for HTTP, 8443 for HTTPS, 9090 for metrics. Document the mapping between external and internal ports clearly.                                                                                           |

#### probes

Health check probes that monitor container health and readiness. Kubernetes uses probes to determine when to route traffic (readiness), restart containers (liveness), and when application has finished starting (startup). Configure probes to prevent routing traffic to unhealthy instances, automatically recover from failures, and handle slow-starting applications. Properly tuned probes improve reliability but aggressive probes can cause false-positive restarts. Missing probes risk serving traffic to broken instances or never recovering from failures. **Production:** Always configure all three probe types. Tune thresholds and timeouts based on actual application behavior under load. Monitor probe failure rates to detect issues early.

##### Properties

| Property    | Type                 | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|-------------|----------------------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `liveness`  | [object](#liveness)  | **Yes**  | Liveness probe determines if the container is alive and functioning. Kubernetes restarts containers when liveness probe fails. Use this to detect and recover from deadlocks, infinite loops, or crashed processes. Failing liveness causes pod restart, which may cause temporary service disruption. Configure this to detect unrecoverable failures that require restart. Too aggressive liveness probes cause restart loops; too lenient probes leave broken containers running. Liveness probe should check if process is responsive, not if it's ready to serve traffic. **Production:** Keep liveness checks simple - verify process responds, not application health. Avoid checking external dependencies (databases, APIs) as their failure shouldn't restart your app. Set conservative thresholds to avoid restart storms. Typical values: initialDelaySeconds 30-60s, intervalSeconds 10s, failureThreshold 3-5. Never restart for transient issues.                                                                                                                                                                                                                 |
| `readiness` | [object](#readiness) | **Yes**  | Readiness probe determines if the container is ready to serve traffic. Kubernetes removes pods from service endpoints when readiness probe fails, stopping traffic routing. Use this to signal when your application is initialized and can handle requests. Failing readiness keeps pod alive but removes it from load balancing. Configure this to check if dependencies (databases, caches) are reachable and application is warmed up. Too aggressive readiness probes cause traffic disruptions; too lenient probes route traffic to unprepared instances. Readiness probe failures don't restart the pod, only remove it from service. **Production:** Check that application is truly ready to serve production traffic. Include dependency checks if failures should stop traffic routing. Set thresholds to avoid flapping (rapidly switching between ready/not-ready). Typical values: initialDelaySeconds 10-30s, intervalSeconds 10s, failureThreshold 3.                                                                                                                                                                                                             |
| `startup`   | [object](#startup)   | **Yes**  | Startup probe determines when the application has successfully started. Kubernetes disables liveness and readiness probes until startup probe succeeds. Use this for slow-starting applications that need more time to initialize than would be reasonable for liveness probes. Startup probe protects slow-starting containers from being killed by liveness probes during initialization. Once startup succeeds, liveness and readiness probes take over. Failing startup probe after all attempts causes container restart. Configure for applications with long initialization (large data loading, complex warmup). Maximum startup time allowed: initialDelaySeconds + (failureThreshold × intervalSeconds). **Production:** Use for applications with startup time >30s or variable startup times. Set generous failureThreshold × intervalSeconds to accommodate slowest startup scenarios. Typical pattern: 60 attempts × 5s interval = 300s (5min) maximum startup time. Once startup completes, normal liveness/readiness probes apply. Without startup probe, slow-starting apps need high liveness initialDelaySeconds which delays failure detection after startup. |

##### liveness

Liveness probe determines if the container is alive and functioning. Kubernetes restarts containers when liveness probe fails. Use this to detect and recover from deadlocks, infinite loops, or crashed processes. Failing liveness causes pod restart, which may cause temporary service disruption. Configure this to detect unrecoverable failures that require restart. Too aggressive liveness probes cause restart loops; too lenient probes leave broken containers running. Liveness probe should check if process is responsive, not if it's ready to serve traffic. **Production:** Keep liveness checks simple - verify process responds, not application health. Avoid checking external dependencies (databases, APIs) as their failure shouldn't restart your app. Set conservative thresholds to avoid restart storms. Typical values: initialDelaySeconds 30-60s, intervalSeconds 10s, failureThreshold 3-5. Never restart for transient issues.

###### Properties

| Property              | Type    | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|-----------------------|---------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `failureThreshold`    | integer | **Yes**  | Consecutive failed liveness probes before restarting container. Provides tolerance for transient issues before taking drastic restart action. Higher values avoid restart loops but leave broken containers running longer. Lower values react quickly but risk unnecessary restarts. Time until restart: failureThreshold × intervalSeconds. Liveness failures trigger restarts (destructive), so threshold should be higher than readiness. **Production:** Use 3-5 for most services (higher than readiness to avoid unnecessary restarts). Use 5-10 for services where restarts are expensive or disruptive. Monitor restart rates - frequent restarts indicate misconfigured probes or application issues. Typical timing: 3-5 failures × 10s interval = 30-50s until restart. |
| `initialDelaySeconds` | integer | **Yes**  | Seconds to wait after container start before beginning liveness probes. Must be longer than application startup time to avoid restarting during initialization. Set this longer than readiness initialDelaySeconds since liveness failures cause restarts. Too short causes restart loops during startup; too long delays detecting crashed containers. For slow-starting apps, use startup probe instead of high liveness initialDelaySeconds. **Production:** Set to time application takes to become responsive (typically 30-60s for web services, 60-120s for Java apps). For apps with variable startup times, use startup probe and keep liveness initialDelaySeconds reasonable. Monitor startup times and adjust based on p95 values.                                      |
| `intervalSeconds`     | integer | **Yes**  | Seconds between liveness probe executions. Determines how quickly Kubernetes detects crashed containers. Shorter intervals detect failures faster but increase overhead. Balance responsiveness with resource usage. Too frequent probes can impact performance; too infrequent delays failure recovery. Liveness probes can be less frequent than readiness probes since restarts are more disruptive. **Production:** Use 10-15s for most services. Use 5s only for critical services requiring fast failure detection. Use 30s for services with expensive health checks. Monitor probe overhead. Keep interval reasonable since restarts are disruptive.                                                                                                                        |
| `timeoutSeconds`      | integer | **Yes**  | Seconds before liveness probe times out and counts as failure. Must be less than intervalSeconds. Set based on expected response time of liveness endpoint. Liveness checks should be very fast (typically <1s). Timeouts indicate potential issues even if they recover. Too short causes false failures; too long delays failure detection. **Production:** Set to 1-3s (liveness checks must be fast). If liveness checks regularly timeout, investigate application performance issues. Never use slow operations in liveness checks. Optimize check logic rather than increasing timeout.                                                                                                                                                                                      |
| `type`                | string  | **Yes**  | Probe mechanism type. `HTTP_GET` makes HTTP requests to an endpoint (best for REST APIs), `TCP` opens TCP connection to a port (for non-HTTP services), `GRPC` calls gRPC health check protocol (for gRPC services). Choose based on your application protocol. For liveness, use simpler checks than readiness - just verify process is responsive. HTTP_GET is most common for web services. **Production:** Use HTTP_GET for REST APIs with simple liveness endpoint (e.g., return 200 OK immediately). Use TCP if no HTTP interface available. Keep liveness checks lightweight - just verify process responds, avoid complex logic or external dependencies. Possible values are: `HTTP_GET`, `TCP`, `GRPC`.                                                                   |

##### readiness

Readiness probe determines if the container is ready to serve traffic. Kubernetes removes pods from service endpoints when readiness probe fails, stopping traffic routing. Use this to signal when your application is initialized and can handle requests. Failing readiness keeps pod alive but removes it from load balancing. Configure this to check if dependencies (databases, caches) are reachable and application is warmed up. Too aggressive readiness probes cause traffic disruptions; too lenient probes route traffic to unprepared instances. Readiness probe failures don't restart the pod, only remove it from service. **Production:** Check that application is truly ready to serve production traffic. Include dependency checks if failures should stop traffic routing. Set thresholds to avoid flapping (rapidly switching between ready/not-ready). Typical values: initialDelaySeconds 10-30s, intervalSeconds 10s, failureThreshold 3.

###### Properties

| Property              | Type    | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|-----------------------|---------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `failureThreshold`    | integer | **Yes**  | Consecutive failed probes before marking container as not ready. Provides tolerance for transient failures before removing from service. Higher values reduce flapping but keep unhealthy pods in service longer. Lower values respond quickly to failures but risk flapping. Determines how long an unhealthy pod serves traffic: failureThreshold × intervalSeconds. For readiness, fails after this many attempts; pod stays alive but stops receiving traffic. **Production:** Use 3 for most services (tolerates brief transient issues). Use 1-2 for services requiring immediate failure detection. Use 5+ for services with occasionally slow but valid responses. Typical timing: 3 failures × 10s interval = 30s until traffic stops.                     |
| `initialDelaySeconds` | integer | **Yes**  | Seconds to wait after container start before beginning probes. Prevents probe failures during application initialization. Set this based on your application's startup time. Too short causes unnecessary failures during startup; too long delays detecting actual failures. For readiness, this should cover basic initialization but not full warmup. Use startup probe for slow-starting apps instead of high initialDelaySeconds. **Production:** Set to minimum time for application to become ready (typically 10-30s for web services, 30-60s for Java apps with large initialization). Monitor startup times and adjust. Use startup probe if initialization time varies significantly.                                                                    |
| `intervalSeconds`     | integer | **Yes**  | Seconds between probe executions. Determines how quickly Kubernetes detects readiness changes. Shorter intervals detect changes faster but increase load on application. Longer intervals reduce overhead but slow failure detection. Balance responsiveness with resource usage. Too frequent probes can impact performance; too infrequent delays traffic routing changes. **Production:** Use 10s for most services (good balance of responsiveness and overhead). Use 5s for critical, fast-recovering services. Use 15-30s for services with expensive health checks. Monitor probe overhead on application performance.                                                                                                                                       |
| `successThreshold`    | integer | **Yes**  | Consecutive successful probes required before marking container as ready. Prevents flapping when readiness status changes frequently. Must be 1 for liveness and startup probes (only configurable for readiness). Higher values provide stability but delay marking containers ready. Set to 1 for fast traffic routing, 2-3 for stability if readiness fluctuates. Most services use 1 to quickly add pods to load balancing. **Production:** Use 1 for stable services to minimize time until pod receives traffic. Use 2-3 if readiness checks have transient failures to avoid flapping. Monitor readiness flapping events to determine appropriate threshold.                                                                                                 |
| `timeoutSeconds`      | integer | **Yes**  | Seconds before probe times out and is considered failed. Must be less than intervalSeconds. Set based on expected response time of health check endpoint. Too short causes false failures on transient slowness; too long delays failure detection. Health check endpoints should be fast (typically <1s response time). Slow health checks indicate potential application issues. **Production:** Set to 1-2s for most services (health checks should be fast). Increase to 3-5s only if health checks involve complex validation. If health checks regularly timeout, optimize the health check logic rather than increasing timeout.                                                                                                                             |
| `type`                | string  | **Yes**  | Probe mechanism type. `HTTP_GET` makes HTTP requests to an endpoint (best for REST APIs), `TCP` opens TCP connection to a port (for non-HTTP services), `GRPC` calls gRPC health check protocol (for gRPC services). Choose based on your application protocol. HTTP_GET provides most information (status code, response body), TCP just checks if port is open, GRPC uses standardized health check. HTTP_GET is most common for web services. **Production:** Use HTTP_GET for REST APIs with dedicated health endpoints. Use TCP for databases or services without HTTP. Use GRPC only if service implements gRPC health checking protocol. Avoid TCP if HTTP is available as it provides less diagnostic info. Possible values are: `HTTP_GET`, `TCP`, `GRPC`. |

##### startup

Startup probe determines when the application has successfully started. Kubernetes disables liveness and readiness probes until startup probe succeeds. Use this for slow-starting applications that need more time to initialize than would be reasonable for liveness probes. Startup probe protects slow-starting containers from being killed by liveness probes during initialization. Once startup succeeds, liveness and readiness probes take over. Failing startup probe after all attempts causes container restart. Configure for applications with long initialization (large data loading, complex warmup). Maximum startup time allowed: initialDelaySeconds + (failureThreshold × intervalSeconds). **Production:** Use for applications with startup time >30s or variable startup times. Set generous failureThreshold × intervalSeconds to accommodate slowest startup scenarios. Typical pattern: 60 attempts × 5s interval = 300s (5min) maximum startup time. Once startup completes, normal liveness/readiness probes apply. Without startup probe, slow-starting apps need high liveness initialDelaySeconds which delays failure detection after startup.

###### Properties

| Property              | Type    | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|-----------------------|---------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `failureThreshold`    | integer | **Yes**  | Number of consecutive probe failures before considering startup failed and restarting container. Determines maximum number of attempts to wait for successful startup. This is the key parameter for accommodating slow/variable startup times. Maximum startup time = initialDelaySeconds + (failureThreshold × intervalSeconds). After this many failures, container is killed and restarted. Higher values accommodate slower or more variable startup times. **Production:** Calculate based on maximum expected startup time. For 5min max startup: 60 failures × 5s interval = 300s. Use p99 startup time from monitoring. Typical values: 30-60 for most services, 100+ for very slow starts (batch processors, ML models). Monitor actual startup times to tune appropriately. |
| `initialDelaySeconds` | integer | **Yes**  | Seconds to wait after container start before first startup probe. Provides buffer before beginning startup checks. Set this lower than liveness initialDelaySeconds since startup probe is designed for slow starts. Can be 0 if you want probes to start immediately. Too high delays startup verification unnecessarily. Combined with failureThreshold × intervalSeconds, determines maximum allowed startup time. **Production:** Use 0-10s for most cases (startup probe handles slow starts via failureThreshold). Only increase if application needs delay before it can even respond to probes. Keep low since startup probe's failureThreshold handles variable startup times.                                                                                                |
| `intervalSeconds`     | integer | **Yes**  | Seconds between startup probe attempts. Determines how frequently Kubernetes checks if startup completed. Balance between responsive startup detection and resource overhead. During startup, probe runs repeatedly until success or failureThreshold reached. Faster intervals detect startup completion quicker. Maximum startup time = initialDelaySeconds + (failureThreshold × intervalSeconds). **Production:** Use 5-10s for most services (frequent enough for reasonable startup detection, infrequent enough to avoid overhead). For very slow starts (minutes), use 10-15s. Monitor startup probe counts to optimize. Example: 10s interval × 60 attempts = 600s max startup.                                                                                               |
| `timeoutSeconds`      | integer | **Yes**  | Seconds before startup probe times out and counts as failure. Must be less than intervalSeconds. Probe must complete within this time. Set based on expected response time even during startup. During initialization, responses may be slower than normal operation. Too short causes false startup failures; too long delays detection. **Production:** Use 2-5s (allow for slower responses during startup). If timeouts are common, investigate startup performance. Keep reasonable but allow for startup-related slowness. Typical: 3-5s for startup vs 1-2s for liveness.                                                                                                                                                                                                       |
| `type`                | string  | **Yes**  | Probe mechanism type for startup check. `HTTP_GET` makes HTTP requests (best for REST APIs), `TCP` opens TCP connection (for non-HTTP services), `GRPC` calls gRPC health check (for gRPC services). Should match your application's interface. Startup probe typically uses same mechanism as liveness/readiness probes. HTTP_GET provides most information during startup. **Production:** Use same probe type as liveness for consistency. HTTP_GET for web services, TCP for services without HTTP. Keep startup check lightweight but verify core initialization is complete. Possible values are: `HTTP_GET`, `TCP`, `GRPC`.                                                                                                                                                     |

#### resources

CPU and memory resource allocation for the container, defining both requests (guaranteed resources) and limits (maximum allowed). Resource configuration determines pod scheduling, node selection, and prevents resource starvation. Configure this based on application profiling and load testing. Requests that are too low cause performance issues; limits that are too low cause OOM kills and throttling. Setting requests = limits provides guaranteed QoS but reduces scheduling flexibility. **Production:** Start with profiled values from staging. Set requests at p50 usage, limits at p95. For critical services, use guaranteed QoS (requests = limits). Monitor actual usage and adjust based on metrics to avoid over-provisioning costs or under-provisioning instability.

##### Properties

| Property   | Type                | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|------------|---------------------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `limits`   | [object](#limits)   | **Yes**  | Maximum resources the container is allowed to use. Kubernetes enforces these limits; exceeding memory limit causes OOM kill, exceeding CPU limit causes throttling. Limits protect the cluster from runaway containers but can cause performance issues if set too low. Without limits, containers can consume all node resources. Setting limits = requests provides guaranteed QoS but less flexibility. Setting limits > requests allows bursting but provides lower QoS guarantees. **Production:** Set limits to p95-p99 observed usage to handle traffic spikes. For critical services, set limits 20-50% above requests to allow bursting. Monitor throttling metrics and OOM kills to tune limits appropriately. |
| `requests` | [object](#requests) | **Yes**  | Guaranteed minimum resources allocated to the container by Kubernetes scheduler. These resources are reserved on the node even if not fully utilized. The scheduler uses requests to determine which nodes can accommodate the pod. Set requests based on baseline resource needs under normal load. Too low causes performance degradation; too high wastes cluster capacity and increases costs. Requests also determine pod QoS class (Guaranteed if requests=limits, Burstable if requests<limits). **Production:** Set requests to p50 (median) observed usage from production profiling. This ensures pods get scheduled on nodes with sufficient capacity while avoiding resource waste.                          |

##### limits

Maximum resources the container is allowed to use. Kubernetes enforces these limits; exceeding memory limit causes OOM kill, exceeding CPU limit causes throttling. Limits protect the cluster from runaway containers but can cause performance issues if set too low. Without limits, containers can consume all node resources. Setting limits = requests provides guaranteed QoS but less flexibility. Setting limits > requests allows bursting but provides lower QoS guarantees. **Production:** Set limits to p95-p99 observed usage to handle traffic spikes. For critical services, set limits 20-50% above requests to allow bursting. Monitor throttling metrics and OOM kills to tune limits appropriately.

###### Properties

| Property | Type   | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|----------|--------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `cpu`    | string | **Yes**  | Maximum CPU cores the container can use (e.g., `1`, `2`, `4`). When exceeded, container is throttled (slowed down), not killed. CPU throttling increases latency and reduces throughput. Set based on maximum expected CPU needs during peak load. Setting this too low causes performance degradation during traffic spikes; too high allows one container to starve others. For latency-sensitive workloads, set limits high enough to avoid throttling. Units: same as requests - whole numbers or decimals, can use `m` suffix. **Production:** Set to p95-p99 CPU usage observed under peak load. For latency-critical services, consider setting limits 2-3x requests to avoid throttling. Monitor CPU throttling metrics.              |
| `memory` | string | **Yes**  | Maximum memory the container can use (e.g., `1Gi`, `2Gi`, `4G`). When exceeded, container is killed with OOM (Out Of Memory) error and restarted. Memory limits are hard boundaries - exceeding them is fatal. Set based on maximum memory footprint under peak load plus safety margin. Too low causes frequent OOM kills and service instability; too high allows memory leaks to impact node stability. Unlike CPU, memory cannot be compressed, so limits must accommodate peak usage. Units: same as requests. **Production:** Set to p95-p99 memory usage with 20-30% safety margin. Monitor OOM kill events. For apps with memory leaks or growing memory, implement regular restarts or fix leaks rather than just increasing limits. |

##### requests

Guaranteed minimum resources allocated to the container by Kubernetes scheduler. These resources are reserved on the node even if not fully utilized. The scheduler uses requests to determine which nodes can accommodate the pod. Set requests based on baseline resource needs under normal load. Too low causes performance degradation; too high wastes cluster capacity and increases costs. Requests also determine pod QoS class (Guaranteed if requests=limits, Burstable if requests<limits). **Production:** Set requests to p50 (median) observed usage from production profiling. This ensures pods get scheduled on nodes with sufficient capacity while avoiding resource waste.

###### Properties

| Property | Type   | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|----------|--------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `cpu`    | string | **Yes**  | Minimum CPU cores guaranteed to the container (e.g., `0.5` = 500 millicores, `2` = 2 cores). CPU requests determine scheduling and share of CPU time when contention occurs. Set this based on application's baseline CPU needs. Too low causes slow response times under load; too high wastes cluster resources and may prevent pod scheduling. CPU is a compressible resource, so pods can use less than requested. Units: whole numbers (cores) or decimals, can use `m` suffix (e.g., `500m` = 0.5 cores). **Production:** Profile CPU usage under normal load. Set requests to p50 CPU usage. For latency-sensitive services, consider p75 to ensure headroom. |
| `memory` | string | **Yes**  | Minimum memory guaranteed to the container (e.g., `512Mi`, `1Gi`, `2G`). Memory requests determine scheduling and guarantee this amount is available. Set based on application's baseline memory footprint. Too low risks OOM kills even with higher limits; too high wastes resources. Memory is incompressible - if app needs more than available, it will be killed. Units: `Mi` (mebibytes, 1024-based), `Gi` (gibibytes), `M` (megabytes, 1000-based), `G` (gigabytes). **Production:** Profile memory usage after app warmup. Set requests to p50 memory usage. For memory-intensive apps, consider p75 to account for gradual memory growth.                  |

#### tags

Custom Kubernetes labels to apply to all created resources (Deployments, Services, Pods, ConfigMaps). Labels enable resource organization, selection, and monitoring queries. Use labels for environment identification (env: production), team ownership (team: platform), cost allocation (cost-center: engineering), or feature flags. Labels must be key-value string pairs following Kubernetes label naming rules. These labels are added to all resources created by this deployment, making them queryable via kubectl and visible in monitoring systems. Change this to add organizational metadata, enable resource grouping, or support monitoring/cost tracking. Labels don't affect functionality but are crucial for operations and cost management. **Production:** Include essential labels: `environment` (prod/staging/dev), `team` (owning team), `service` (service name), `version` (app version), `cost-center` (for billing). Use consistent label keys across organization. Avoid high-cardinality values in labels (like timestamps, user IDs). Example: `{"environment": "production", "team": "platform", "service": "api", "criticality": "high"}`. Labels enable queries like `kubectl get pods -l environment=production,team=platform` and Prometheus queries `up{environment="production"}`.

| Property | Type | Required | Description |
|----------|------|----------|-------------|

#### tolerations

##### Properties

| Property   | Type   | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|------------|--------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `effect`   | string | No       | The taint effect to tolerate. `NoSchedule` prevents new pods from scheduling, `PreferNoSchedule` avoids scheduling but allows if necessary, `NoExecute` evicts running pods that don't tolerate. Matching the effect type determines when the toleration applies. Use `NoSchedule` for dedicated nodes, `NoExecute` for critical workload isolation. **Default:** `NoSchedule` (pods won't schedule without toleration). **Production:** Use `NoSchedule` for dedicated workloads, `NoExecute` for strict isolation where incompatible pods must be evicted. Avoid `PreferNoSchedule` in production as it provides weak guarantees. Possible values are: `NoSchedule`, `PreferNoSchedule`, `NoExecute`. |
| `key`      | string | No       | The taint key that this toleration matches (e.g., `node.kubernetes.io/spot`, `dedicated`). This identifies which node taint the pod can tolerate. Change this to match specific taints applied to your nodes. Matching the correct key allows pods to be scheduled on nodes with that taint. **Production:** Use consistent taint keys across your cluster. Common patterns: `workload-type=batch`, `instance-type=spot`, `node-role=gpu`.                                                                                                                                                                                                                                                              |
| `operator` | string | No       | How to match the taint. `Equal` matches exact key-value pairs, while `Exists` matches any value for the key. Use `Equal` when you need precise matching (e.g., only spot instances with specific characteristics). Use `Exists` for broader matching (e.g., any spot instance). **Default:** `Equal` (requires value to be specified). **Production:** Use `Equal` for explicit control, `Exists` for flexible scheduling across node types. Possible values are: `Equal`, `Exists`.                                                                                                                                                                                                                    |
| `value`    | string | No       | The taint value that must match when operator is `Equal` (e.g., `true`, `spot`, `gpu`). This provides fine-grained control over which tainted nodes the pod can tolerate. Change this to match specific taint values on your nodes. Required when operator is `Equal`, ignored when operator is `Exists`. **Production:** Use meaningful values that indicate node characteristics (e.g., `true` for dedicated nodes, `spot` for spot instances).                                                                                                                                                                                                                                                       |



### Running Application

* Create an Intellij Run configuration
* Pass operation name as command line argument
* Pass following environment variables
  * `COMPONENT_METADATA`: [componentMetadata.json](../example/internal_aws_container/componentMetadata.json)
  * `CONFIG`: merged json of [base_config.json](../example/internal_aws_container/base_config.json) and [flavour_config.json](../example/internal_aws_container/flavour_config.json). In the case of operation [operation_config.json](../example/internal_aws_container/operation_config.json)
